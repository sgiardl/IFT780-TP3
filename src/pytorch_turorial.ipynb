{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel pytorch - TP3 - IFT725\n",
    "\n",
    "Tel que mentionné dans l'énoncé du travail, vous devez recopier les blocs de code du tutoriel suivant\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "en donnant, pour chaque bloc, une description en format \"markdown\" de son contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Warm-up: numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Numpy\"** est une bibliothèque qui prend en charge des tableaux et matrices, ainsi qu'une multitude de fonctions mathématiques opérant sur ces tableaux. Donc, elle n'est pas destinée pour implémenter des modèles d'apprentissage automatique. Toutefois, le code suivant est un exemple qui montre qu'on peut implémenter un modèle de **régression polynomiale** d’ordre 3 capable de prédire la valeur de la fonction **sinus**.\n",
    "D'abord, on commence par définir les variables \"x\" et \"y\" (données d'entraînement) qui représentent l'entrée et la sortie cible du modèle, \"a\", \"b\", \"c\" et \"d\" qui sont les poids à apprendre par le modèle (initialisés aléatoirement) et \"y_pred\" qui est la prédiction du réseau. Le  processus d'apprentissage s'effectue par la minimisation d'une fonction d'erreur **\"Distance euclidienne\"** à travers un algorithme d'optimisation différentiable **\"Descente de gradient\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **x** : ndarray à 1 dimension de taille 2000 d'éléments de type float64. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle.\n",
    "- **y** : ndarray à 1 dimension de taille 2000 d'éléments de type float64. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle.\n",
    "- **a** : float64. C'est le premier poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{0}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **b** : float64. C'est le deuxième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{1}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **c** : float64. C'est le troisième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{2}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **d** : float64. C'est le quatrième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{3}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **y_pred** : ndarray à 1 dimension de taille 2000 d'éléments de type float64. Elle comprend les prédictions de modèle. Son calcul s'effectue lors de la forward pass.\n",
    "- **loss** : float64. C'est la fonction de perte du modèle \"Distance euclidienne\". Son calcul s'effectue lors de la forward pass.\n",
    "- **grad_y_pred** : ndarray à 1 dimension de taille 2000 d'éléments de type float64. Elle comprend la dérivée de la fonction d'erreur par rapport à la variable de prédiction **y_pred**: $\\frac {dL}{dy_{pred}}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_a** : float64. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **a**: $\\frac {dL}{da}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_b** : float64. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **b**: $\\frac {dL}{db}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_c** : float64. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **c**: $\\frac {dL}{dc}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_d** : float64. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **d**: $\\frac {dL}{dd}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de la descente du gradient, fixé à 1e-6. Il contrôle combien les poids peuvent changer au cours de chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La bibliothèque **\"Numpy\"** est incapable d'effectuer des calculs sur des processeurs graphiques. Ce code nous introduit à un nouveau type d'objets: **les tenseurs**. Maintenant, on entraine comme l'étape précédente un modèle de **régression polynomiale** d’ordre 3 capable de prédire la valeur de la fonction **sinus** mais on remplace les **ndarray** de la bibliothèque **\"Numpy\"** par des **tensors** de la bibliothèque **\"Pytorch\"**. Cette dernière nous permet d'entraîner le modèle sur un **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dtype** : torch.dtype, fixé à torch.float. C'est un objet qui représente le type de données d'une torch.Tensor.\n",
    "- **device** : torch.device. C'est un objet représentant l'appareil sur lequel une torch.Tensor est ou sera allouée (CPU ou GPU). selon ce code, les torch.Tensor peuvent être envoyés au CPU ou GPU (cuda:0), ce qui signifie \"type='cuda', index=0\".\n",
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle.\n",
    "- **a** : torch.FloatTensor, rang 0. C'est le premier poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{0}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **b** : torch.FloatTensor, rang 0. C'est le deuxième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{1}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **c** : torch.FloatTensor, rang 0. C'est le troisième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{2}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **d** : torch.FloatTensor, rang 0. C'est le quatrième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{3}$ du polynôme) et calculé lors de la forward pass.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle. Son calcul s'effectue lors de la forward pass.\n",
    "- **loss** : float. C'est la fonction de perte du modèle \"Distance euclidienne\". Son calcul s'effectue lors de la forward pass.\n",
    "- **grad_y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend la dérivée de la fonction d'erreur par rapport à la variable de prédiction **y_pred**: $\\frac {dL}{dy_{pred}}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_a** : torch.FloatTensor, rang 0. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **a**: $\\frac {dL}{da}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_b** : torch.FloatTensor, rang 0. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **b**: $\\frac {dL}{db}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_c** : torch.FloatTensor, rang 0. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **c**: $\\frac {dL}{dc}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **grad_d** : torch.FloatTensor, rang 0. C'est la dérivée partielle de la fonction d'erreur par rapport au coefficient **d**: $\\frac {dL}{dd}$. Son calcul s'effectue lors de la backward pass.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de la descente du gradient, fixé à 1e-6. Il contrôle combien les poids peuvent changer au cours de chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implémentation précédente présente une implémentation manuelle du forward et backward pass d'un réseau de neurones. En effet, durant le forward pass, on définit un graphe de calcul dans lequel les noeuds sont des tenseurs et les arêtes sont des fonctions. Ensuite, **Autograd** fournit une différenciation automatique qui automatise le calcul des gradients au cours du backward pass. \n",
    "Dans ce code, on entraine un modèle de **régression polynomiale** d’ordre 3 capable de prédire la valeur de la fonction **sinus** tout en profitant du package **Autograd**. A chaque itération, un None est affecté aux gradients calculés pour éviter leurs accumulations dans **.grad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dtype** : torch.dtype, fixé à torch.float. C'est un objet qui représente le type de données d'une torch.Tensor.\n",
    "- **device** : torch.device. C'est un objet représentant l'appareil sur lequel une torch.Tensor est ou sera allouée (CPU ou GPU). selon ce code, les torch.Tensor peuvent être envoyés au CPU ou GPU (cuda:0), ce qui signifie \"type='cuda', index=0\".\n",
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle. Elle est définie lors du forward pass avec un paramètre **requires_grad=False** par défaut car on n'a pas besoin de calculer son gradient lors du backward pass.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle. Elle est définie lors du forward pass avec un paramètre **requires_grad=False** par défaut car on n'a pas besoin de calculer son gradient lors du backward pass.\n",
    "- **a** : torch.FloatTensor, rang 0. C'est le premier poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{0}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **b** : torch.FloatTensor, rang 0. C'est le deuxième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{1}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **c** : torch.FloatTensor, rang 0. C'est le troisième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{2}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **d** : torch.FloatTensor, rang 0. C'est le quatrième poids du modèle initialisé par une valeur aléatoire (c'est le coefficient de $x^{3}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle. Elle est définie lors du forward pass.\n",
    "- **loss** : torch.FloatTensor, rang 0. C'est la fonction de perte du modèle \"Distance euclidienne\". Aussi, la ligne **loss.backward()** nous permet de calculer les gradients des paramètres.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de la descente du gradient, fixé à 1e-6. Il contrôle combien les poids peuvent changer au cours de chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code nous permet d'implémenter une nouvelle fonction autograd personnalisée. En effet, il définit une sous-classe nommée **LegendrePolynomial3** dont laquelle les méthodes forward et backward sont associées au **Polynôme de Legendre**. Le but du modèle est d'entrainer un polynôme d’ordre 3 qui prédit la fonction **sinus**. Il est défini comme suit:\n",
    "$$ y = a + b P_3 ( c + dx ) $$ avec $P_3(x) = 12 (5x^3−3x)$, Polynôme de Legendre\n",
    "\n",
    "A chaque itération, un None est affecté aux gradients calculés pour éviter leurs accumulations dans **.grad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LegendrePolynomial3** : torch.autograd.function.FunctionMeta. C'est une sous-classe qui nous permet d'avoir une fonction autograd personnalisée.\n",
    "- **P3** : builtin_function_or_method. C'est un alias de la méthode Function.apply (LegendrePolynomial3.apply dans cet exemple).\n",
    "- **dtype** : torch.dtype, fixé à torch.float. C'est un objet qui représente le type de données d'une torch.Tensor.\n",
    "- **device** : torch.device. C'est un objet représentant l'appareil sur lequel une torch.Tensor est ou sera allouée (CPU ou GPU). selon ce code, les torch.Tensor peuvent être envoyés au CPU ou GPU (cuda:0), ce qui signifie \"type='cuda', index=0\".\n",
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle. Elle est définie lors du forward pass avec un paramètre **requires_grad=False** par défaut car on n'a pas besoin de calculer son gradient lors du backward pass.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle. Elle est définie lors du forward pass avec un paramètre **requires_grad=False** par défaut car on n'a pas besoin de calculer son gradient lors du backward pass.\n",
    "- **a** : torch.FloatTensor, rang 0. C'est le premier poids du modèle initialisé par **0.0** pour assurer la convergence du modèle vers la bonne valeur (c'est le coefficient de $x^{0}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **b** : torch.FloatTensor, rang 0. C'est le deuxième poids du modèle initialisé par **-1.0** pour assurer la convergence du modèle vers la bonne valeur (c'est le coefficient de $x^{1}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **c** : torch.FloatTensor, rang 0. C'est le troisième poids du modèle initialisé par **0.0** pour assurer la convergence du modèle vers la bonne valeur (c'est le coefficient de $x^{2}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **d** : torch.FloatTensor, rang 0. C'est le quatrième poids du modèle initialisé par **0.3** pour assurer la convergence du modèle vers la bonne valeur (c'est le coefficient de $x^{3}$ du polynôme) et calculé lors de la forward pass. Elle est définie lors du forward pass avec un paramètre **requires_grad=True** car on a besoin de calculer son gradient lors du backward pass.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle. Elle est définie lors du forward pass.\n",
    "- **loss** : torch.FloatTensor, rang 0. C'est la fonction de perte du modèle \"Distance euclidienne\". Aussi, la ligne **loss.backward()** nous permet de calculer les gradients des paramètres.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de la descente du gradient, fixé à 5e-6. Il contrôle combien les poids peuvent changer au cours de chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code nous introduit au package **nn**. Ce dernier sert à définir les couches du réseau de neurones sous la forme de modules. Un module reçoit un **tenseur d'entrée**, calcule un **tenseur de sortie** et maintient un état interne tel que les **tenseurs de paramètres**. Le but du modèle est encore l'entrainement d'un polynôme d'ordre 3 pour prédire la fonction sinus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle.\n",
    "- **p** : torch.Tensor, rang 1, de taille 3 d'éléments de type int64. Elle comprend les exposants du polynôme à entrainer.\n",
    "- **xx** : torch.Tensor, rang 2, de taille (2000, 3) d'éléments de type float32. Elle nous représente un tenseur (x, x^2, x^3).\n",
    "- **model** : torch.nn.modules.container.Sequential. Cette variable définit l'architecture du réseau de neurones. Elle se compose de deux couches: **Linear(in_features=3, out_features=1, bias=True)** qui est une couche linéaire qui reçoit 3 tenseurs d'entrée et calcule un en sortie (avec **bias=True**) et **Flatten(start_dim=0, end_dim=1)** qui représente une couche d'aplatissement qui reçoit le tenseur de sortie de la couche linéaire et l'applatit en un tenseur de rang 1, de taille 2000. A chaque itération, la mise des gradients à zéro s'effectue par **model.zero_grad()**.\n",
    "- **loss_fn** : torch.nn.modules.loss.MSELoss. C'est la fonction de perte du modèle \"**Mean Squared Error Loss** (MSELoss)\". On fixe le paramètre **reduction** de la fonction **torch.nn.MSELoss** à la valeur **'sum'** pour éviter la division de la loss par le nombre d'éléments (2000).\n",
    "- **loss** : torch.FloatTensor, rang 0. Elle comprend le résultat de calcul de la fonction d'erreur **loss_fn**. Aussi, la ligne **loss.backward()** nous permet de calculer les gradients des paramètres.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle tel que **y_pred = model(xx)**.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de la descente du gradient, fixé à 1e-6. Il contrôle combien les poids peuvent changer au cours de chaque itération.\n",
    "- **param** : torch.nn.parameter.Parameter, rang 1, avec un seul élément de type float32.\n",
    "- **linear_layer** : torch.nn.modules.linear.Linear. Cette variable contient les éléments de la première couche du réseau de neurones, **Linear(in_features=3, out_features=1, bias=True)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PyTorch: optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au cours de l'implémentation précédente, la mise à jour des gradients des paramètres a été effectuée manuellement à travers la commande **torch.no_grad()**. Le code suivant utilisera le package **optim** vu qu'il offre plusieurs algorithmes d'optimisation prédéfinis. Alors, on va entrainer le même modèle précédent avec les pacages **nn** et **optim** en utilisant l'algorithme d'optimisation **RMSprop**. Le but du modèle est encore l'entrainement d'un polynôme d'ordre 3 pour prédire la fonction sinus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle.\n",
    "- **p** : torch.Tensor, rang 1, de taille 3 d'éléments de type int64. Elle comprend les exposants du polynôme à entrainer.\n",
    "- **xx** : torch.Tensor, rang 2, de taille (2000, 3) d'éléments de type float32. Elle nous représente un tenseur (x, x^2, x^3).\n",
    "- **model** : torch.nn.modules.container.Sequential. Cette variable définit l'architecture du réseau de neurones. Elle se compose de deux couches: **Linear(in_features=3, out_features=1, bias=True)** qui est une couche linéaire qui reçoit 3 tenseurs d'entrée et calcule un en sortie (avec **bias=True**) et **Flatten(start_dim=0, end_dim=1)** qui représente une couche d'aplatissement qui reçoit le tenseur de sortie de la couche linéaire et l'applatit en un tenseur de rang 1, de taille 2000. A chaque itération, la mise des gradients à zéro s'effectue par **model.zero_grad()**.\n",
    "- **loss_fn** : torch.nn.modules.loss.MSELoss. C'est la fonction de perte du modèle \"**Mean Squared Error Loss** (MSELoss)\". On fixe le paramètre **reduction** de la fonction **torch.nn.MSELoss** à la valeur **'sum'** pour éviter la division de la loss par le nombre d'éléments (2000).\n",
    "- **loss** : torch.FloatTensor, rang 0. Elle comprend le résultat de calcul de la fonction d'erreur **loss_fn**. Aussi, la ligne **loss.backward()** nous permet de calculer les gradients des paramètres.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle tel que **y_pred = model(xx)**.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage.\n",
    "- **learning_rate** : float. C'est le taux d'apprentissage de l'algorithme d'apprentissage utilisé **RMSprop**, fixé à 1e-3. Il contrôle combien les poids peuvent changer au cours de chaque itération.\n",
    "- **optimizer** : torch.optim.rmsprop.RMSprop. C'est un optimiseur qui utilise l'algorithme **\"RMSProp\"**. On implémente la ligne **optimizer.zero_grad()** pour mettre les gradients à zéro avant le backward pass et **optimizer.step()** pour mettre à jour les paramètres du modèle.\n",
    "- **linear_layer** : torch.nn.modules.linear.Linear. Cette variable contient les éléments de la première couche du réseau de neurones, **Linear(in_features=3, out_features=1, bias=True)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnement global du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code nous permet d'implémenter un nouveau module **nn.Module** personnalisée. En effet, il définit une sous-classe nommée **Polynomial3** dont laquelle la méthode forward utilise un type de module et d'autograd personnalisé. Le but du modèle est encore l'entrainement d'un polynôme d'ordre 3 pour prédire la fonction sinus avec une \"Squared Error Loss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Polynomial3** : C'est une sous-classe qui nous permet d'avoir une fonction **nn.Module** personnalisée. Ses attributs $a$, $b$, $c$, et $d$ sont les paramètres du modèle. Ils sont aléatoirement initialisés. La méthode **forward** prend en entrée le tenseur **x** et retourne un tenseur calculé se lon l'équation suivante:\n",
    "$$a+bx+cx^2+dx^3$$\n",
    "Aussi, la méthode **string** retourne une chaine de caractères **\"y = a + b x + c x^2 + d x^3\"**.\n",
    "- **x** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend 2000 valeurs équidistantes comprises entre $- \\pi$ et $\\pi$ à utiliser pour l'apprentissage du modèle.\n",
    "- **y** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les valeurs cibles $sin(x)$ à prédire par le modèle.\n",
    "- **model** : C'est une instance de la clasee **Polynomial3**. \n",
    "- **criterion** : torch.nn.modules.loss.MSELoss. C'est la fonction de perte du modèle \"**Squared Error Loss**\" (car on fixe le paramètre **reduction** de la fonction **torch.nn.MSELoss** à la valeur **'sum'** pour éviter la division de la loss par le nombre d'éléments (2000)).\n",
    "- **optimizer** : torch.optim.sgd.SGD. C'est un optimiseur qui utilise l'algorithme **\"descente de gradient\"**. On implémente la ligne **optimizer.zero_grad()** pour mettre les gradients à zéro avant le backward pass et **optimizer.step()** pour mettre à jour les paramètres du modèle.\n",
    "- **y_pred** : torch.FloatTensor, rang 1, de taille 2000 d'éléments de type float32. Elle comprend les prédictions de modèle tel que **y_pred = model(x)**.\n",
    "- **loss** : torch.FloatTensor, rang 0. Elle comprend le résultat de calcul de la fonction d'erreur **loss_fn**. Aussi, la igne ** loss = criterion(y_pred, y)** nous permet de calculer l'erreur et la ligne **loss.backward()** nous permet de calculer les gradients des paramètres.\n",
    "- **t** : int. C'est une variable comprise entre 0 et 1999, elle représente le compteur d'itérations dans la boucle d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
    "# module which is members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
